{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo for the Calculation of the Semantic Brand Score - Basic Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Down the Rabbit-Hole. Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it\n"
     ]
    }
   ],
   "source": [
    "# Read text documents from an example CSV file\n",
    "import csv\n",
    "readfile = csv.reader(open(\"AliceWonderland.csv\", 'rt',  encoding=\"utf8\"), delimiter = \"|\", quoting=csv.QUOTE_NONE)\n",
    "texts = [line[0] for line in readfile]\n",
    "#4 Chapters of Alice in Wonderland\n",
    "print(len(texts))\n",
    "print(texts[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I imported the random text file in Python as a list of text documents (texts), which are processed to remove punctuation, stop-words and special characters. Words are lowercased and split into tokens, thus obtaining a new texts variable, which is a list of lists. More complex operations of text preprocessing are always possible (such as the removal of html tags or ‘#’), for which I recommend reading one of many tutorials on Natural Language Processing in Python. The stopwords list is taken from the NLTK package. Lastly, word affixes are remove through Snowball Stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rabbit', 'hole', 'alice', 'begin', 'get', 'tire']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import re, string and nltk, and download stop-words\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Define stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "stopw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Define brands (lowercase)\n",
    "brands = ['alice', 'rabbit']\n",
    "\n",
    "# texts is a list of strings, one for each document analyzed.\n",
    "\n",
    "#Convert to lowercase\n",
    "texts = [t.lower() for t in texts]\n",
    "#Remove words that start with HTTP\n",
    "texts = [re.sub(r\"http\\S+\", \" \", t) for t in texts]\n",
    "#Remove words that start with WWW\n",
    "texts = [re.sub(r\"www\\S+\", \" \", t) for t in texts]\n",
    "#Remove punctuation\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "texts = [regex.sub(' ', t) for t in texts]\n",
    "#Remove words made of single letters\n",
    "texts = [re.sub(r'\\b\\w{1}\\b', ' ', t) for t in texts]\n",
    "#Remove stopwords\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopw) + r')\\b\\s*')\n",
    "texts = [pattern.sub(' ', t) for t in texts]\n",
    "#Remove additional whitespaces\n",
    "texts = [re.sub(' +',' ',t) for t in texts]\n",
    "\n",
    "#Tokenize text documents (becomes a list of lists)\n",
    "texts = [t.split() for t in texts]\n",
    "\n",
    "# Snowball Stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "texts = [[stemmer.stem(w) if w not in brands else w for w in t] for t in texts]\n",
    "texts[0][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During text preprocessing we should pay attention not to lose useful information. Smileys :-), made of punctuation, can be very important if we calculate sentiment.\n",
    "We can now proceed with the calculation of prevalence, which counts the frequency of occurrence of each brand name — subsequently standardized considering the scores of all the words in the texts. My choice of standardization here is to subtract the mean and divide by the standard deviation. Other approaches are also possible. This step is important to compare measures carried out considering different time frames or sets of documents (e.g. brand importance on Twitter in April and May). Normalization of absolute scores is necessary before summing prevalence, diversity and connectivity to obtain the Semantic Brand Score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence alice 6.716253083416179\n",
      "Prevalence rabbit 0.23502089447171487\n"
     ]
    }
   ],
   "source": [
    "#PREVALENCE\n",
    "#Import Counter and Numpy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "#Create a dictionary with frequency counts for each word\n",
    "countPR = Counter()\n",
    "for t in texts:\n",
    "    countPR.update(Counter(t))\n",
    "\n",
    "#Calculate average score and standard deviation\n",
    "avgPR = np.mean(list(countPR.values()))\n",
    "stdPR = np.std(list(countPR.values()))\n",
    "\n",
    "#Calculate standardized Prevalence for each brand\n",
    "PREVALENCE = {}\n",
    "for brand in brands:\n",
    "    PR_brand = (countPR[brand] - avgPR) / stdPR\n",
    "    PREVALENCE[brand] = PR_brand\n",
    "    print(\"Prevalence\", brand, PR_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next and most important step is to transform texts (list of lists of tokens) into a social network where nodes are words and links are weighted according to the number of co-occurrences between each pair of words. In this step we have to define a co-occurrence range, i.e. a maximum distance between co-occurring words (here is set to 3). In addition, we might want to remove links which represent negligible co-occurrences, for example those of weight = 1. Sometimes it can also be useful to remove isolates, if these are not brands.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Network\n",
      "No. of Nodes: 519 No. of Edges: 1514\n"
     ]
    }
   ],
   "source": [
    "#Import Networkx\n",
    "import networkx as nx\n",
    "\n",
    "#Choose a co-occurrence range\n",
    "co_range = 3\n",
    "\n",
    "#Create an undirected Network Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "#Each word is a network node\n",
    "nodes = set([item for sublist in texts for item in sublist])\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "#Add links based on co-occurrences\n",
    "for doc in texts:\n",
    "    w_list = []\n",
    "    length= len(doc)\n",
    "    for k, w in enumerate(doc):\n",
    "        #Define range, based on document length\n",
    "        if (k+co_range) >= length:\n",
    "            superior = length\n",
    "        else:\n",
    "            superior = k+co_range+1\n",
    "        #Create the list of co-occurring words\n",
    "        if k < length-1:\n",
    "            for i in range(k+1,superior):\n",
    "                linked_word = doc[i].split()\n",
    "                w_list = w_list + linked_word\n",
    "        #If the list is not empty, create the network links\n",
    "        if w_list:    \n",
    "            for p in w_list:\n",
    "                if G.has_edge(w,p):\n",
    "                    G[w][p]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(w, p, weight=1)\n",
    "        w_list = []\n",
    "\n",
    "#Remove negligible co-occurrences based on a filter\n",
    "link_filter = 2\n",
    "#Create a new Graph which has only links above\n",
    "#the minimum co-occurrence threshold\n",
    "G_filtered = nx.Graph() \n",
    "G_filtered.add_nodes_from(G)\n",
    "for u,v,data in G.edges(data=True):\n",
    "    if data['weight'] >= link_filter:\n",
    "        G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "\n",
    "#Optional removal of isolates\n",
    "isolates = set(nx.isolates(G_filtered))\n",
    "isolates -= set(brands)\n",
    "G_filtered.remove_nodes_from(isolates)\n",
    "\n",
    "#Check the resulting graph (for small test graphs)\n",
    "#G_filtered.nodes()\n",
    "#G_filtered.edges(data = True)\n",
    "print(\"Filtered Network\\nNo. of Nodes:\", G_filtered.number_of_nodes(), \"No. of Edges:\", G_filtered.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having determined the co-occurrence network, we can now calculate diversity and connectivity, which are degree centrality and betweenness centrality of a brand node. We standardize these values as we did with prevalence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity alice 5.594199479718734\n",
      "Diversity rabbit -0.13888252606943657\n"
     ]
    }
   ],
   "source": [
    "#DIVERSITY\n",
    "DIVERSITY_sequence=dict(nx.degree(G_filtered))\n",
    "#Calculate average score and standard deviation\n",
    "avgDI = np.mean(list(DIVERSITY_sequence.values()))\n",
    "stdDI = np.std(list(DIVERSITY_sequence.values()))\n",
    "#Calculate standardized Diversity for each brand\n",
    "DIVERSITY = {}\n",
    "for brand in brands:\n",
    "    DI_brand = (DIVERSITY_sequence[brand] - avgDI) / stdDI\n",
    "    DIVERSITY[brand] = DI_brand\n",
    "    print(\"Diversity\", brand, DI_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we calculate connectivity as weighted betweenness centraliy, we first have to define inverse weights, as weights are treated by Networkx as distances (which is the opposite of our case).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connectivity alice 1.270318490240101\n",
      "Connectivity rabbit 0.1026273469656663\n"
     ]
    }
   ],
   "source": [
    "#Define inverse weights \n",
    "for u,v,data in G_filtered.edges(data=True):\n",
    "    if 'weight' in data and data['weight'] != 0:\n",
    "        data['inverse'] = 1/data['weight']\n",
    "    else:\n",
    "        data['inverse'] = 1   \n",
    "\n",
    "#CONNECTIVITY\n",
    "CONNECTIVITY_sequence=nx.betweenness_centrality(G_filtered, normalized=False, weight ='inverse')\n",
    "#Calculate average score and standard deviation\n",
    "avgCO = np.mean(list(CONNECTIVITY_sequence.values()))\n",
    "stdCO = np.std(list(CONNECTIVITY_sequence.values()))\n",
    "#Calculate standardized Prevalence for each brand\n",
    "CONNECTIVITY = {}\n",
    "for brand in brands:\n",
    "    CO_brand = (CONNECTIVITY_sequence[brand] - avgCO) / stdCO\n",
    "    CONNECTIVITY[brand] = CO_brand\n",
    "    print(\"Connectivity\", brand, CO_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Semantic Brand Score of each brand is finally obtained by summing the standardized values of prevalence, diversity and connectivity. Different approaches are also possible, such as taking the geometric mean of unstandardized coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBS alice 13.580771053375013\n",
      "SBS rabbit 0.1987657153679446\n",
      "SBS:  {'alice': 13.580771053375013, 'rabbit': 0.1987657153679446}\n"
     ]
    }
   ],
   "source": [
    "#Obtain the Semantic Brand Score of each brand\n",
    "SBS = {}\n",
    "for brand in brands:\n",
    "    SBS[brand] = PREVALENCE[brand] + DIVERSITY[brand] + CONNECTIVITY[brand]\n",
    "    print(\"SBS\", brand, SBS[brand])\n",
    "print(\"SBS: \",SBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREVALENCE</th>\n",
       "      <th>DIVERSITY</th>\n",
       "      <th>CONNECTIVITY</th>\n",
       "      <th>SBS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>6.716253</td>\n",
       "      <td>5.594199</td>\n",
       "      <td>1.270318</td>\n",
       "      <td>13.580771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rabbit</th>\n",
       "      <td>0.235021</td>\n",
       "      <td>-0.138883</td>\n",
       "      <td>0.102627</td>\n",
       "      <td>0.198766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PREVALENCE  DIVERSITY  CONNECTIVITY        SBS\n",
       "alice     6.716253   5.594199      1.270318  13.580771\n",
       "rabbit    0.235021  -0.138883      0.102627   0.198766"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a final pandas data frame with all results\n",
    "import pandas as pd\n",
    "\n",
    "PREVALENCE = pd.DataFrame.from_dict(PREVALENCE, orient=\"index\", columns = [\"PREVALENCE\"])\n",
    "DIVERSITY = pd.DataFrame.from_dict(DIVERSITY, orient=\"index\", columns = [\"DIVERSITY\"])\n",
    "CONNECTIVITY = pd.DataFrame.from_dict(CONNECTIVITY, orient=\"index\", columns = [\"CONNECTIVITY\"])\n",
    "SBS = pd.DataFrame.from_dict(SBS, orient=\"index\", columns = [\"SBS\"])\n",
    "\n",
    "SBS = pd.concat([PREVALENCE, DIVERSITY, CONNECTIVITY, SBS], axis=1, sort=False)\n",
    "SBS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
